{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Leaves\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----STAR-----\n",
      "try to link with DataBase:cate_test......\n",
      "sccessful link\n",
      "getting data sccessful\n",
      "the csv document has created \n",
      "processing...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.928 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----FINISH-----\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar  1 19:09:26 2017\n",
    "\n",
    "@author: user\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pyodbc\n",
    "import jieba\n",
    "\n",
    "from math import*\n",
    "import codecs\n",
    "from nltk import word_tokenize\n",
    "import os #讀取程式目前路徑用\n",
    "import sys #為了能夠吃進主程式外部輸進的參數\n",
    "import nltk #引入自然語言處理模組\n",
    "import io #檔案io\n",
    "from nltk.corpus import stopwords #去無用字\n",
    "import re #需要用正則表示式移除標點與數字\n",
    "from nltk.corpus import wordnet as wn #2013/10/26改用wordnet來做字根還原\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import string\n",
    "import csv\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "print(\"-----STAR-----\")\n",
    "print(\"try to link with DataBase:cate_test......\")\n",
    "cnxn = pyodbc.connect(\"DRIVER={SQL Server};SERVER=LEAVES-PC\\SA000;DATABASE=cate_test;UID=sa;PWD=00000000\")\n",
    "cursor = cnxn.cursor()\n",
    "print(\"sccessful link\")\n",
    "\n",
    "cursor.execute(\"select [BulletinPlatform] from [cate_test].[dbo].[20161020001_all_candinates]\")\n",
    "results = cursor.fetchall()\n",
    "\n",
    "\n",
    "count=0 \n",
    "for i in results:\n",
    "    count+=1\n",
    "    #print ('%d:%s' % (count, i))\n",
    "print(\"getting data sccessful\")\n",
    "\n",
    "def getDocDistance(a, b):\n",
    "    if LA.norm(a)==0 or LA.norm(b)==0:\n",
    "        return -1  \n",
    "    return round(np.inner(a, b) / (LA.norm(a) * LA.norm(b)), 5) \n",
    "def getDocSimilarity(wordFrequencyPair, minTimes=1):\n",
    "    #v1 = var1\n",
    "    #v2 = var2\n",
    "    result = 0\n",
    "    try:\n",
    "        result = getDocDistance(v1, v2)\n",
    "    except(RuntimeError, TypeError, NameError):\n",
    "        pass        \n",
    "    return result\n",
    "\n",
    "with open('new_PFP_20161020001.csv', 'w', newline='') as f: \n",
    "    spamwriter = csv.writer(f, dialect='excel')\n",
    "    headers = ['i', 'j', 'Similarity','i text','j text']\n",
    "    spamwriter.writerow(headers)\n",
    "    print(\"the csv document has created \")\n",
    "    print(\"processing...........\")\n",
    "    for i in range(0 , len(results)):\n",
    "        for j in range(i+1 ,len(results)):      \n",
    "            for word1 in results[i]:                      \n",
    "                deletepunc = [u'〔',u'〕',u'〝',u'〞',u'『 ',u'』',u'〈',u'〉',u'\\\\',u'（', u'）',u'～',u'u3000',u'\"',u'&gt',u'&lt',u';',u'，', u'。', u'、', u'；', u'：', u'?', u'「', u'」', u'%', u'.', u',', u'？', u'-', u'~',u'!',u'！', u'&nbsp', u'<BR>', u'“', u'”', u'【', u'】', u'《', u'》',u'：',u'●',u'(',u')',u'；']\n",
    "                for chin in deletepunc:\n",
    "                    word1 = word1.replace(chin, ' ')\n",
    "                result= jieba.cut(word1) \n",
    "                result1 = \" \".join(result)\n",
    "                file1 = nltk.word_tokenize(result1)\n",
    "                trainTokenFrequency = nltk.FreqDist(file1)              \n",
    "            for word2 in results[j]:             \n",
    "                deletepunc = [u'〔',u'〕',u'〝',u'〞',u'『 ',u'』',u'〈',u'〉',u'\\\\',u'（', u'）',u'～',u'u3000',u'\"',u'&gt',u'&lt',u';',u'，', u'。', u'、', u'；', u'：', u'?', u'「', u'」', u'%', u'.', u',', u'？', u'-', u'~',u'!',u'！', u'&nbsp', u'<BR>', u'“', u'”', u'【', u'】', u'《', u'》',u'：',u'●',u'(',u')',u'；']\n",
    "                for chin in deletepunc:\n",
    "                    word2 = word2.replace(chin, ' ')\n",
    "                result= jieba.cut(word2) \n",
    "                result2 = \" \".join(result)\n",
    "                file2 = nltk.word_tokenize(result2)\n",
    "                testTokenFrequency = nltk.FreqDist(file2) \n",
    "            \n",
    "            wordFrequencyPair = [trainTokenFrequency, testTokenFrequency]\n",
    "            \n",
    "            minTimes=1\n",
    "            dict1 = {}\n",
    "            for key in wordFrequencyPair[0].keys():\n",
    "                if wordFrequencyPair[0].get(key, 0) >= minTimes:\n",
    "                    dict1[key] = wordFrequencyPair[0].get(key, 0)\n",
    "            dict2 = {}\n",
    "            for key in wordFrequencyPair[1].keys():\n",
    "                if wordFrequencyPair[1].get(key, 0) >= minTimes:\n",
    "                    dict2[key] = wordFrequencyPair[1].get(key, 0)\n",
    "            for key in dict2.keys():\n",
    "                if dict1.get(key, 0) == 0:   #第一個字典中如果沒有第二字典中的字就把沒有的字加進去並設為0\n",
    "                    dict1[key] = 0\n",
    "            for key in dict1.keys():\n",
    "                if dict2.get(key, 0) == 0:\n",
    "                    dict2[key] = 0\n",
    "            v1 = []\n",
    "            for w1 in sorted(dict1.keys()):\n",
    "                v1.append(dict1.get(w1))\n",
    "            v2 = []    \n",
    "            for w2 in sorted(dict2.keys()):\n",
    "                v2.append(dict2.get(w2))\n",
    "                \n",
    "            def getDocSimilarity(wordFrequencyPair, minTimes=1):\n",
    "                result = 0\n",
    "                try:                \n",
    "                    result = getDocDistance(v1, v2)\n",
    "                    #print(\"getDocDistance try\")\n",
    "                except(RuntimeError, TypeError, NameError):\n",
    "                    pass\n",
    "                return result    \n",
    "            data = [( i , j , getDocSimilarity(wordFrequencyPair, 1),result1,result2)]   \n",
    "            spamwriter.writerows(data)       #寫入csv\n",
    "print(\"-----FINISH-----\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
