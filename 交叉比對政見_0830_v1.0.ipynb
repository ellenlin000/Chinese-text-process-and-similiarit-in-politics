{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Leaves\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create all the dictionary and list....\n",
      "*sccessful create the custom dictionary*\n",
      "import the customer dictionary....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.955 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*sccessful create all the dictionary and list*\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyodbc\n",
    "import jieba\n",
    "\n",
    "from math import*\n",
    "import codecs\n",
    "from nltk import word_tokenize\n",
    "import os #讀取程式目前路徑用\n",
    "import sys #為了能夠吃進主程式外部輸進的參數\n",
    "import nltk #引入自然語言處理模組\n",
    "import io #檔案io\n",
    "from nltk.corpus import stopwords #去無用字\n",
    "import re #需要用正則表示式移除標點與數字\n",
    "from nltk.corpus import wordnet as wn #2013/10/26改用wordnet來做字根還原\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import string\n",
    "import csv\n",
    "from __future__ import print_function, unicode_literals\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "print('create all the dictionary and list....')\n",
    "###建立同義詞詞典(戴老師指定:key為完整編號，value為所有詞)\n",
    "dict_data_full_number_policy = {}\n",
    "dict_key_2 = {}\n",
    "dict_key_2['whatever']=\"just_for_test\"\n",
    "with open('full_number_policy.txt' , encoding = 'utf8') as df:\n",
    "    for kv in [d.strip().split('\t') for d in df]:\n",
    "        #print('kv[0]=',kv[0])\n",
    "        if kv[0] not in dict_key_2['whatever']:\n",
    "            dict_key_2['whatever'] = kv[0]      \n",
    "        #print('kv[1]=',kv[1])\n",
    "        #print()\n",
    "        dict_data_full_number_policy.setdefault(kv[0], [])\n",
    "        dict_data_full_number_policy[kv[0]].append(kv[1])\n",
    "\n",
    "###建立同義詞詞典(戴老師指定:key為所有詞，value為同義詞編號)\n",
    "dict_data_key_in_policy = {}\n",
    "dict_key_1 = {}\n",
    "dict_key_1['whatever']=\"just_for_test\"\n",
    "with open('key_in_policy.txt' , encoding = 'utf8') as df:\n",
    "    for kv in [d.strip().split('\t') for d in df]:\n",
    "        #print('kv[0]=',kv[0])\n",
    "        if kv[0] not in dict_key_1['whatever']:\n",
    "            dict_key_1['whatever'] = kv[0]      \n",
    "        #print('kv[1]=',kv[1])\n",
    "        #print()\n",
    "        dict_data_key_in_policy.setdefault(kv[0], [])\n",
    "        dict_data_key_in_policy[kv[0]].append(kv[1])\n",
    "\n",
    "###建立同義詞詞典\n",
    "dict_data = {}\n",
    "dict_key = {}\n",
    "dict_key['abc']=\"aaaaaa\"\n",
    "with open('split_cate.txt' , encoding = 'utf8') as df:\n",
    "    for kv in [d.strip().split('\t') for d in df]:\n",
    "        #print('kv[0]=',kv[0])\n",
    "        if kv[0] not in dict_key['abc']:\n",
    "            dict_key['abc'] = kv[0]      \n",
    "        #print('kv[1]=',kv[1])\n",
    "        #print()\n",
    "        dict_data.setdefault(kv[0], [])\n",
    "        dict_data[kv[0]].append(kv[1])\n",
    "        \n",
    "aaa='A0707\\nA2203\\nA1402\\nA0705\\nA2202\\nA2313\\nA0711\\nA0203\\nA2602\\nA0101\\nA0713\\nA1504\\nA1906\\nA1506\\nA2604\\nA2314\\nA1705\\nA2321\\nA2408\\nA1702\\nA0403\\nA2309\\nA2325\\nA0621\\nA2101\\nA2407\\nA2002\\nA2711\\nA1905\\nA2503\\nA2703\\nA0906\\nA0704\\nA1202\\nA0505\\nA2902\\nA0626\\nA2509\\nA2310\\nA1902\\nA3101\\nA2418\\nA0604\\nA1803\\nA0607\\nA0625\\nA2205\\nA0801\\nA2710\\nA0703\\nA2718\\nA2403\\nA2415\\nA0714\\nA2716\\nA2304\\nA2606\\nA1007\\nA2312\\nA3102\\nA2701\\nA1801\\nA1401\\nA0401\\nA0605\\nA2712\\nA1203\\nA2417\\nA0623\\nA2416\\nA0708\\nA0504\\nA0502\\nA0202\\nA1508\\nA2324\\nA1505\\nA0622\\nA2502\\nA1512\\nA1008\\nA2720\\nA1802\\nA2507\\nA0620\\nA2705\\nA1201\\nA2904\\nA3103\\nA2802\\nA0102\\nA1406\\nA1407\\nA1502\\nA2504\\nA2302\\nA2406\\nA2003\\nA0614\\nA0802\\nA2104\\nA2204\\nA0610\\nA0506\\nA0501\\nA2103\\nA0402\\nA2317\\nA2419\\nA0615\\nA2108\\nA1507\\nA1404\\nA2201\\nA1602\\nA2605\\nA0613\\nA2501\\nA0619\\nA2315\\nA1403\\nA2401\\nA2301\\nA2106\\nA0903\\nA2702\\nA2323\\nA2414\\nA2305\\nA2206\\nA2105\\nA0507\\nA2717\\nA1004\\nA1601\\nA0715\\nA2001\\nA1002\\nA2413\\nA2506\\nA1703\\nA1701\\nA2402\\nA0902\\nA2322\\nA1005\\nA2303\\nA2316\\nA2409\\nA1901\\nA0712\\nA1510\\nA1301\\nA0612\\nA1501\\nA2107\\nA2901\\nA2704\\nA0709\\nA1603\\nA3001\\nA1101\\nA2903\\nA2318\\nA2405\\nA2410\\nA0608\\nA2308\\nA0609\\nA2412\\nA0601\\nA2510\\nA2306\\nA1509\\nA0901\\nA1302\\nA0301\\nA2601\\nA0907\\nA0602\\nA0706\\nA2319\\nA1704\\nA2004\\nA0618\\nA0201\\nA2607\\nA2505\\nA2420\\nA1511\\nA1503\\nA1003\\nA1804\\nA2307\\nA1405\\nA2411\\nA0702\\nA0616\\nA2326\\nA1001\\nA1904\\nA0905\\nA1604\\nA2512\\nA2803\\nA2708\\nA0904\\nA0503\\nA1006\\nA0710\\nA2714\\nA0624\\nA2719\\nA2320\\nA1303\\nA2715\\nA2404\\nA2804\\nA2707\\nA2706\\nA2713\\nA2511\\nA0617\\nA2801\\nA0611\\nA2508\\nA2311\\nA0204\\nA1605\\nA2709\\nA2603\\nA1903\\nA0606\\nA0603\\nA2102\\nA0701'\n",
    "dict_data_key = aaa.split('\\n')\n",
    "dict_data_key.sort()\n",
    "\n",
    "f= open('diction_split.txt', encoding = 'utf8')#讀所有詞\n",
    "s = f.read()\n",
    "#print(s)\n",
    "spilt_s = s.split(\"\\n\")#分所有詞\n",
    "del spilt_s[287] #刪最後一個空的\n",
    "print('*sccessful create the custom dictionary*')\n",
    "\n",
    "#連接自定義字典\n",
    "print('import the customer dictionary....')\n",
    "jieba.load_userdict(\"dict_policy.txt\")\n",
    "print('*sccessful create all the dictionary and list*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to link with DataBase:cate_test......\n",
      "*sccessful link*\n"
     ]
    }
   ],
   "source": [
    "##連結資料庫\n",
    "\n",
    "print(\"try to link with DataBase:cate_test......\")\n",
    "cnxn = pyodbc.connect(\"DRIVER={SQL Server};SERVER=LEAVES-PC\\SA000;DATABASE=cate_test;UID=sa;PWD=00000000\")\n",
    "cursor = cnxn.cursor()\n",
    "print(\"*sccessful link*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up the data.......\n",
      "*sccessful set up*\n",
      "setting up the data.......\n",
      "*sccessful set up*\n"
     ]
    }
   ],
   "source": [
    "#選擇資料集\n",
    "\n",
    "cursor.execute(\"select [BulletinPlatform] from [cate_test].[dbo].[X_loser_candinates_20161020001]\" )\n",
    "#建立資料集\n",
    "print('setting up the data.......')\n",
    "results_1 = cursor.fetchall()\n",
    "print('*sccessful set up*')\n",
    "\n",
    "cursor.execute(\"select [BulletinPlatform] from [cate_test].[dbo].[X_loser_candinates_20161020001]\" )\n",
    "#建立資料集\n",
    "print('setting up the data.......')\n",
    "results_2 = cursor.fetchall()\n",
    "print('*sccessful set up*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def delpunc_n_jieba_n_tok(re): \n",
    "    count=0\n",
    "    for i in re:\n",
    "        count+=1\n",
    "        #print ('%d: %s' % (count, i))\n",
    "    for times in range(0,count):\n",
    "        chineseFilter1 =  [u'．',u'的',u'〔',u'〕',u'〝',u'〞',u'『 ',u'』',u'〈',u'〉',u'\\\\',u'（', u'）',u'～',u'u3000',u'\"',u'&gt',u'&lt',u';',u'，', u'。', u'、', u'；', u'：', u'?', u'「', u'」', u'%', u'.', u',', u'？', u'-', u'~',u'!',u'！', u'&nbsp', u'<BR>', u'“', u'”', u'【', u'】', u'《', u'》',u'：',u'●',u'(',u')',u'；']\n",
    "        for chin in chineseFilter1:\n",
    "            re[times] = str(re[times])#轉為字串才能處理\n",
    "            re[times] = re[times].replace(chin,'')\n",
    "        re_cut = jieba.cut(re[times])\n",
    "        re_cut_show = \" \".join(re_cut)##空白一定要有不然她無法斷後面的token\n",
    "        re_token = nltk.word_tokenize(re_cut_show)\n",
    "        re[times] = re_token\n",
    "\n",
    "delpunc_n_jieba_n_tok(results_1)\n",
    "delpunc_n_jieba_n_tok(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def new_gene_list(new_gene_list_r):\n",
    "    new_gene=[]\n",
    "    for cani_num in range(0,len(new_gene_list_r)):\n",
    "        #print('now cadina=',new_gene_list_r[cani_num])\n",
    "        tem_gene=[]\n",
    "        for i in range(0,len(new_gene_list_r[cani_num])):\n",
    "                for j in range(0,len(spilt_s)):\n",
    "                    if spilt_s[j] == new_gene_list_r[cani_num][i]:\n",
    "                        tem_gene.append(spilt_s[j])\n",
    "        new_gene.append(tem_gene)\n",
    "        #print('現在的new_gene->',new_gene)\n",
    "    return new_gene\n",
    "\n",
    "new_gene_1 = new_gene_list(results_1)\n",
    "new_gene_2 = new_gene_list(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_frequent_of_key(k_li):\n",
    "    key_freq_list=[]\n",
    "    for k_li_num in k_li:\n",
    "        k_li_tem_freq = nltk.FreqDist(k_li_num)\n",
    "        key_freq_list.append(k_li_tem_freq)\n",
    "    return key_freq_list \n",
    "        \n",
    "key_freq_list_1 = count_frequent_of_key(new_gene_1)\n",
    "key_freq_list_2 = count_frequent_of_key(new_gene_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_todict_freq_to_dict(k_f_list):\n",
    "    ues_this_k_real_list=[]\n",
    "    for k_f_list_dic_freq in range(0,len(k_f_list)):\n",
    "        k_real_freq={}\n",
    "        k_real_freq.update(k_f_list[k_f_list_dic_freq])\n",
    "        ues_this_k_real_list.append(k_real_freq)\n",
    "    return ues_this_k_real_list\n",
    "        \n",
    "ues_this_k_real_list_1 = convert_todict_freq_to_dict(key_freq_list_1)\n",
    "ues_this_k_real_list_2 = convert_todict_freq_to_dict(key_freq_list_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_word_to_number_in_key(word_list):\n",
    "    key_w='0'\n",
    "    BKUP_each_key_in_each_list='0'\n",
    "    real_list_remove_for_freq = []\n",
    "    real_list_remove_for_freq_1 = []\n",
    "    real_remove_pun_for_freq = {}\n",
    "    for k_list_in_word in word_list:\n",
    "        for each_key_in_each_list in k_list_in_word:\n",
    "            BKUP_each_key_in_each_list = each_key_in_each_list\n",
    "            if each_key_in_each_list in dict_data_key_in_policy.keys():\n",
    "                each_k_repea = str(dict_data_key_in_policy[each_key_in_each_list])\n",
    "                real_remove_pun_for_freq.setdefault(each_k_repea[2:7],[])#弄個List的容器給她裝整數\n",
    "                if each_k_repea[2:7] in real_remove_pun_for_freq.keys():\n",
    "                    real_remove_pun_for_freq[each_k_repea[2:7]].append(k_list_in_word[BKUP_each_key_in_each_list])###注意迴圈k_list_in_word\n",
    "                else:\n",
    "                    real_remove_pun_for_freq.update({each_k_repea[2:7]:k_list_in_word[BKUP_each_key_in_each_list]})\n",
    "            else:\n",
    "                print('pass')\n",
    "        backup_real_remove_pun_for_freq = real_remove_pun_for_freq.copy()    ####複製.copy()\n",
    "        #print('backup_real_remove_pun_for_freq->',backup_real_remove_pun_for_freq)\n",
    "        real_remove_pun_for_freq.clear()\n",
    "        real_list_remove_for_freq_1.append(backup_real_remove_pun_for_freq)\n",
    "    return real_list_remove_for_freq_1\n",
    "\n",
    "real_list_remove_for_freq_1_1 = convert_word_to_number_in_key(ues_this_k_real_list_1)              \n",
    "real_list_remove_for_freq_1_2 = convert_word_to_number_in_key(ues_this_k_real_list_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###把>1的數字全部變成1\n",
    "def turn_num_all_to_1(real_list_remove_for_freq_1):\n",
    "    for swp_real_list_remove in real_list_remove_for_freq_1:\n",
    "        for swp_key_find in swp_real_list_remove.keys() :\n",
    "            for num_loop_in_swp_real in range(0,len(swp_real_list_remove[swp_key_find])):\n",
    "                if swp_real_list_remove[swp_key_find][num_loop_in_swp_real] >1:##在第幾個因為他每個數字都是list\n",
    "                    swp_real_list_remove[swp_key_find][num_loop_in_swp_real] = 1\n",
    "    return real_list_remove_for_freq_1\n",
    "\n",
    "real_list_turn_num_all_to_1_1 = turn_num_all_to_1(real_list_remove_for_freq_1_1)\n",
    "real_list_turn_num_all_to_1_2 = turn_num_all_to_1(real_list_remove_for_freq_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##弄向量出來\n",
    "\n",
    "def generate_vector(g_v):\n",
    "    t_fina_vector = []\n",
    "    t_buff_fina_vector=[]\n",
    "    for n_g_v in g_v:\n",
    "        for dic_data_num in dict_data_key:\n",
    "            if dic_data_num in n_g_v.keys():\n",
    "                t_buff_fina_vector.append(n_g_v[dic_data_num][0])\n",
    "            else:\n",
    "                t_buff_fina_vector.append(0)\n",
    "        copy_t_buff_fina_vector = t_buff_fina_vector[:]\n",
    "        t_fina_vector.append(copy_t_buff_fina_vector)\n",
    "        t_buff_fina_vector[:] = []\n",
    "    return t_fina_vector\n",
    "\n",
    "vector_1 = generate_vector(real_list_turn_num_all_to_1_1)\n",
    "vector_2 = generate_vector(real_list_turn_num_all_to_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leaves\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py:326: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "\n",
    "#dataSet = [[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
    "def math_fuction_of_simi(vec_c_1,vec_c_2):\n",
    "    vec_consin_result = 1 - spatial.distance.cosine(vec_c_1, vec_c_2)\n",
    "    #print('in math_fuction_of_simi', vec_consin_result)\n",
    "    vec_consin_result = round(vec_consin_result,4)\n",
    "    return vec_consin_result\n",
    "\n",
    "f_txt = open('x_x_try.txt', 'w', encoding = 'UTF-8')\n",
    "\n",
    "\n",
    "def write_simi_in_txt(ready_in_vec):\n",
    "    data = (ready_in_vec)\n",
    "    #print('type是',type(data))\n",
    "    #print(data)\n",
    "    f_txt.write(str(data))\n",
    "    f_txt.write('\\n')\n",
    "    \n",
    "def count_similarity(vec_list_1, vec_list_2):\n",
    "    for vec_1 in vec_list_1:\n",
    "        #print('1',vec_1)\n",
    "        for vec_2 in vec_list_2:\n",
    "            #print('2',vec_2)\n",
    "            get_return = math_fuction_of_simi(vec_1, vec_2)\n",
    "            #print(get_return)\n",
    "            write_simi_in_txt(get_return)\n",
    "\n",
    "\n",
    "    \n",
    "#test_consine_result = 1 - spatial.distance.cosine(dataSetI, dataSetIII)\n",
    "#test_consine_result\n",
    "count_similarity(vector_1, vector_2)\n",
    "f_txt.close()\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
