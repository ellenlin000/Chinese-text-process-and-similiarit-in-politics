{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----STAR-----\n",
      "try to link with DataBase:cate_test......\n",
      "sccessful link\n",
      "getting data sccessful\n",
      "the csv document has created \n",
      "processing...........\n",
      "-----FINISH-----\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar  1 19:09:26 2017\n",
    "\n",
    "@author: user\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pyodbc\n",
    "import jieba\n",
    "\n",
    "from math import*\n",
    "import codecs\n",
    "from nltk import word_tokenize\n",
    "import os #讀取程式目前路徑用\n",
    "import sys #為了能夠吃進主程式外部輸進的參數\n",
    "import nltk #引入自然語言處理模組\n",
    "import io #檔案io\n",
    "from nltk.corpus import stopwords #去無用字\n",
    "import re #需要用正則表示式移除標點與數字\n",
    "from nltk.corpus import wordnet as wn #2013/10/26改用wordnet來做字根還原\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import string\n",
    "import csv\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "print(\"-----STAR-----\")\n",
    "print(\"try to link with DataBase:cate_test......\")\n",
    "cnxn = pyodbc.connect(\"DRIVER={SQL Server};SERVER=LEAVES-PC\\SA000;DATABASE=cate_test;UID=sa;PWD=00000000\")\n",
    "cursor = cnxn.cursor()\n",
    "print(\"sccessful link\")\n",
    "\n",
    "cursor.execute(\"select [BulletinPlatform] from [cate_test].[dbo].[X_loser_candinates_20161020001]\")\n",
    "results = cursor.fetchall()\n",
    "\n",
    "\n",
    "count=0 \n",
    "for i in results:\n",
    "    count+=1\n",
    "    #print ('%d:%s' % (count, i))\n",
    "print(\"getting data sccessful\")\n",
    "\n",
    "def getDocDistance(a, b):\n",
    "    if LA.norm(a)==0 or LA.norm(b)==0:\n",
    "        return -1  \n",
    "    return round(np.inner(a, b) / (LA.norm(a) * LA.norm(b)), 5) \n",
    "def getDocSimilarity(wordFrequencyPair, minTimes=1):\n",
    "    #v1 = var1\n",
    "    #v2 = var2\n",
    "    result = 0\n",
    "    try:\n",
    "        result = getDocDistance(v1, v2)\n",
    "    except(RuntimeError, TypeError, NameError):\n",
    "        pass        \n",
    "    return result\n",
    "\n",
    "with open('new_without_dict_x_x.csv', 'w', newline='') as f: \n",
    "    spamwriter = csv.writer(f, dialect='excel')\n",
    "    headers = ['i', 'j', 'Similarity','i text','j text']\n",
    "    spamwriter.writerow(headers)\n",
    "    print(\"the csv document has created \")\n",
    "    print(\"processing...........\")\n",
    "    for i in range(0 , len(results)):\n",
    "        for j in range(i+1 ,len(results)):      \n",
    "            for word1 in results[i]:                      \n",
    "                deletepunc = [u'〔',u'〕',u'〝',u'〞',u'『 ',u'』',u'〈',u'〉',u'\\\\',u'（', u'）',u'～',u'u3000',u'\"',u'&gt',u'&lt',u';',u'，', u'。', u'、', u'；', u'：', u'?', u'「', u'」', u'%', u'.', u',', u'？', u'-', u'~',u'!',u'！', u'&nbsp', u'<BR>', u'“', u'”', u'【', u'】', u'《', u'》',u'：',u'●',u'(',u')',u'；']\n",
    "                for chin in deletepunc:\n",
    "                    word1 = word1.replace(chin, ' ')\n",
    "                result= jieba.cut(word1) \n",
    "                result1 = \" \".join(result)\n",
    "                file1 = nltk.word_tokenize(result1)\n",
    "                trainTokenFrequency = nltk.FreqDist(file1)              \n",
    "            for word2 in results[j]:             \n",
    "                deletepunc = [u'〔',u'〕',u'〝',u'〞',u'『 ',u'』',u'〈',u'〉',u'\\\\',u'（', u'）',u'～',u'u3000',u'\"',u'&gt',u'&lt',u';',u'，', u'。', u'、', u'；', u'：', u'?', u'「', u'」', u'%', u'.', u',', u'？', u'-', u'~',u'!',u'！', u'&nbsp', u'<BR>', u'“', u'”', u'【', u'】', u'《', u'》',u'：',u'●',u'(',u')',u'；']\n",
    "                for chin in deletepunc:\n",
    "                    word2 = word2.replace(chin, ' ')\n",
    "                result= jieba.cut(word2) \n",
    "                result2 = \" \".join(result)\n",
    "                file2 = nltk.word_tokenize(result2)\n",
    "                testTokenFrequency = nltk.FreqDist(file2) \n",
    "            \n",
    "            wordFrequencyPair = [trainTokenFrequency, testTokenFrequency]\n",
    "            \n",
    "            minTimes=1 \n",
    "            dict1 = {}\n",
    "            for key in wordFrequencyPair[0].keys():\n",
    "                if wordFrequencyPair[0].get(key, 0) >= minTimes:\n",
    "                    dict1[key] = wordFrequencyPair[0].get(key, 0)\n",
    "            dict2 = {}\n",
    "            for key in wordFrequencyPair[1].keys():\n",
    "                if wordFrequencyPair[1].get(key, 0) >= minTimes:\n",
    "                    dict2[key] = wordFrequencyPair[1].get(key, 0)\n",
    "            for key in dict2.keys():\n",
    "                if dict1.get(key, 0) == 0:   #第一個字典中如果沒有第二字典中的字就把沒有的字加進去並設為0\n",
    "                    dict1[key] = 0\n",
    "            for key in dict1.keys():\n",
    "                if dict2.get(key, 0) == 0:\n",
    "                    dict2[key] = 0\n",
    "            v1 = []\n",
    "            for w1 in sorted(dict1.keys()):\n",
    "                v1.append(dict1.get(w1))\n",
    "            v2 = []    \n",
    "            for w2 in sorted(dict2.keys()):\n",
    "                v2.append(dict2.get(w2))\n",
    "                \n",
    "            def getDocSimilarity(wordFrequencyPair, minTimes=1):\n",
    "                result = 0\n",
    "                try:                \n",
    "                    result = getDocDistance(v1, v2)\n",
    "                    #print(\"getDocDistance try\")\n",
    "                except(RuntimeError, TypeError, NameError):\n",
    "                    pass\n",
    "                return result    \n",
    "            data = [( i , j , getDocSimilarity(wordFrequencyPair, 1),result1,result2)]   \n",
    "            spamwriter.writerows(data)       #寫入csv\n",
    "print(\"-----FINISH-----\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "try to link with DataBase:cate_test......\n",
      "sccessful link\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "######################我是備份\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Mar  1 19:09:26 2017\n",
    "\n",
    "@author: user\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pyodbc\n",
    "import jieba\n",
    "\n",
    "from math import*\n",
    "import codecs\n",
    "from nltk import word_tokenize\n",
    "import os #讀取程式目前路徑用\n",
    "import sys #為了能夠吃進主程式外部輸進的參數\n",
    "import nltk #引入自然語言處理模組\n",
    "import io #檔案io\n",
    "from nltk.corpus import stopwords #去無用字\n",
    "import re #需要用正則表示式移除標點與數字\n",
    "from nltk.corpus import wordnet as wn #2013/10/26改用wordnet來做字根還原\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import string\n",
    "import csv\n",
    "from pandas import Series, DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "#連接資料庫\n",
    "print(\"try to link with DataBase:cate_test......\")\n",
    "cnxn = pyodbc.connect(\"DRIVER={SQL Server};SERVER=LEAVES-PC\\SA000;DATABASE=cate_test;UID=sa;PWD=00000000\")\n",
    "cursor = cnxn.cursor()\n",
    "print(\"sccessful link\")\n",
    "\n",
    "\n",
    "#印出前十筆資料(指定在dbo.mobile01_list資料表)\n",
    "\n",
    "cursor.execute(\"select [BulletinPlatform] from dbo.KMT_20161020001\")\n",
    "#row = cursor.fetchone()\n",
    "\n",
    "results = cursor.fetchall()\n",
    "\n",
    "v1 = []\n",
    "v2 = []\n",
    "\n",
    "count=0 \n",
    "  \n",
    "for i in results:\n",
    "    count+=1\n",
    "    #print ('%d:%s' % (count, i))\n",
    "   \n",
    "\n",
    "    \n",
    "#def word1andword2similarity(wordFrequencyPair):\n",
    "\n",
    "def getDocDistance(a, b):\n",
    "    if LA.norm(a)==0 or LA.norm(b)==0:\n",
    "        return -1\n",
    "    \n",
    "    return round(np.inner(a, b) / (LA.norm(a) * LA.norm(b)), 5)\n",
    "    \n",
    "\n",
    "def getDocSimilarity(wordFrequencyPair, minTimes=1):\n",
    "    #v1 = var1\n",
    "    #v2 = var2\n",
    "    result = 0\n",
    "    \n",
    "    try:\n",
    "        result = getDocDistance(v1, v2)\n",
    "    except(RuntimeError, TypeError, NameError):\n",
    "        pass\n",
    "        \n",
    "    return result\n",
    "       \n",
    "'''   \n",
    "print('-----------------------------------------------------------' + '\\n')     \n",
    "\n",
    "for word1 in results[1]:      \n",
    "    print('WORD1:' , word1)\n",
    "    result= jieba.cut(word1) \n",
    "    #print('result:' , result)\n",
    "    result1 = \" \".join(result)\n",
    "    print('result1:' , result1)\n",
    "    file1 = nltk.word_tokenize(result1)\n",
    "    print(\"file1: \", file1)\n",
    "    trainTokenFrequency = nltk.FreqDist(file1)   \n",
    "    print(\"trainTokenFrequency1: \", trainTokenFrequency)\n",
    "    \n",
    "    \n",
    "for word2 in results[2]:      \n",
    "    print('WORD2:' , word2)\n",
    "    result= jieba.cut(word2) \n",
    "    #print('result:' , result)\n",
    "    result2 = \" \".join(result)\n",
    "    print('result2:' , result2)\n",
    "    file2 = nltk.word_tokenize(result2)\n",
    "    print(\"file2: \", file2)\n",
    "    testTokenFrequency = nltk.FreqDist(file2) \n",
    "    print(\"testTokenFrequency2: \", testTokenFrequency)  \n",
    "'''\n",
    "\n",
    "\n",
    "with open('new_NP_20161020001.csv', 'w', newline='') as f: \n",
    "    spamwriter = csv.writer(f, dialect='excel')\n",
    "    headers = ['i', 'j', 'Similarity','i text','j text']\n",
    "    spamwriter.writerow(headers)\n",
    "    for i in range(0 , len(results)):\n",
    "        for j in range(i+1 ,len(results)):\n",
    "                    \n",
    "            ##print(\"************************\")\n",
    "            ##print(\"i= \", i, \"j= \",  j)\n",
    "            \n",
    "        \n",
    "            for word1 in results[i]:  \n",
    "                              \n",
    "                deletepunc = [u'\\\\',u'（', u'）',u'～',u'u3000',u'\"',u'&gt',u'&lt',u';',u'，', u'。', u'、', u'；', u'：', u'?', u'「', u'」', u'%', u'.', u',', u'？', u'-', u'~',u'!',u'！', u'&nbsp', u'<BR>', u'“', u'”', u'【', u'】', u'《', u'》',u'：',u'●',u'(',u')',u'；']\n",
    "                for chin in deletepunc:\n",
    "                    word1 = word1.replace(chin, ' ')\n",
    "                ##print('remove puncation File1: ' + '\\n' + word1)\n",
    "                \n",
    "                #print('WORD1:' ,word1)\n",
    "                result= jieba.cut(word1) \n",
    "                #print('result:' , result)\n",
    "                result1 = \" \".join(result)\n",
    "                ######print('result1:' , result1)\n",
    "                file1 = nltk.word_tokenize(result1)\n",
    "                #print(\"file1: \", file1)\n",
    "                trainTokenFrequency = nltk.FreqDist(file1)   \n",
    "                #print(\"trainTokenFrequency1: \", trainTokenFrequency)\n",
    "            \n",
    "            \n",
    "            for word2 in results[j]:   \n",
    "                \n",
    "                deletepunc = [u'\\\\',u'（', u'）',u'～',u'u3000',u'\"',u'&gt',u'&lt',u';',u'，', u'。', u'、', u'；', u'：', u'?', u'「', u'」', u'%', u'.', u',', u'？', u'-', u'~',u'!',u'！', u'&nbsp', u'<BR>', u'“', u'”', u'【', u'】', u'《', u'》',u'：',u'●',u'(',u')',u'；']\n",
    "                for chin in deletepunc:\n",
    "                    word2 = word2.replace(chin, ' ')\n",
    "                ##print('remove puncation File2: ' + '\\n' + word2)\n",
    "                \n",
    "                #print('WORD2:' , word2)\n",
    "                result= jieba.cut(word2) \n",
    "                #print('result:' , result)\n",
    "                result2 = \" \".join(result)\n",
    "                ######print('result2:' , result2)\n",
    "                file2 = nltk.word_tokenize(result2)\n",
    "                #print(\"file2: \", file2)\n",
    "                testTokenFrequency = nltk.FreqDist(file2) \n",
    "                #print(\"testTokenFrequency2: \", testTokenFrequency)\n",
    "\n",
    "            #print(\"trainTokenFrequency:\",trainTokenFrequency)\n",
    "            #print(\"testTokenFrequency\", testTokenFrequency)\n",
    "            wordFrequencyPair = [trainTokenFrequency, testTokenFrequency]\n",
    "            #print(\"wordFrequencyPair[0]\",wordFrequencyPair[0])\n",
    "            #print(\"wordFrequencyPair[1]\",wordFrequencyPair[1])\n",
    "            #print(\"wordFrequencyPair\", wordFrequencyPair)\n",
    "            #word1andword2similarity(wordFrequencyPair1)\n",
    "\n",
    "            #print(wordFrequencyPair1)\n",
    "            minTimes=1\n",
    "            dict1 = {}\n",
    "            for key in wordFrequencyPair[0].keys():\n",
    "                if wordFrequencyPair[0].get(key, 0) >= minTimes:\n",
    "                    dict1[key] = wordFrequencyPair[0].get(key, 0)\n",
    "\n",
    "            dict2 = {}\n",
    "            for key in wordFrequencyPair[1].keys():\n",
    "                if wordFrequencyPair[1].get(key, 0) >= minTimes:\n",
    "                    dict2[key] = wordFrequencyPair[1].get(key, 0)\n",
    "\n",
    "            for key in dict2.keys():\n",
    "                if dict1.get(key, 0) == 0:   #第一個字典中如果沒有第二字典中的字就把沒有的字加進去並設為0\n",
    "                    dict1[key] = 0\n",
    "\n",
    "            for key in dict1.keys():\n",
    "                if dict2.get(key, 0) == 0:\n",
    "                    dict2[key] = 0\n",
    "\n",
    "            v1 = []\n",
    "            for w1 in sorted(dict1.keys()):\n",
    "                v1.append(dict1.get(w1))\n",
    "                #print (\"(1)\", w1, dict1.get(w1))\n",
    "\n",
    "            v2 = []    \n",
    "            for w2 in sorted(dict2.keys()):\n",
    "                v2.append(dict2.get(w2))\n",
    "                #print (\"(2)\", w2, dict2.get(w2))\n",
    "\n",
    "\n",
    "            #print('-----------------------------------------------------------' + '\\n') \n",
    "\n",
    "            def getDocSimilarity(wordFrequencyPair, minTimes=1):\n",
    "                #v1 = var1\n",
    "                #v2 = var2\n",
    "                result = 0\n",
    "\n",
    "                try:                \n",
    "                    result = getDocDistance(v1, v2)\n",
    "                    #print(\"getDocDistance try\")\n",
    "                except(RuntimeError, TypeError, NameError):\n",
    "                    pass\n",
    "\n",
    "                return result\n",
    "\n",
    "            ##print('Cosine similarity ', getDocSimilarity(wordFrequencyPair, 1))\n",
    "            ##print('-------------------------')\n",
    "            #print('-----------------------------------------------------------' + '\\n') \n",
    "     \n",
    "            data = [( i , j , getDocSimilarity(wordFrequencyPair, 1),result1,result2)]   \n",
    "            spamwriter.writerows(data)       #寫入csv\n",
    "\n",
    "print(\"OK\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
